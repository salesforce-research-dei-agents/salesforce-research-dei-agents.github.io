<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Salesforce Research DEI Agents</title>

    <meta name="description" content="Salesforce Research DEI Agents">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <meta property="og:image" content="./img/overview.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1196">
    <meta property="og:image:height" content="705">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://salesforce-research-dei-agents.github.io/"/>
    <meta property="og:title" content="Salesforce Research DEI Agents" />
    <meta property="og:description" content="Project page for Salesforce Research DEI Agents." />
 
    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Salesforce Research DEI Agents" />
    <meta name="twitter:description" content="Project page for Salesforce Research DEI Agents" />
    <meta name="twitter:image" content="./img/overview.jpg" />

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/leaderboard.css">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JBS73F70V"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5JBS73F70V');
</script>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b><font size="+6">Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents</font></b></br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li><a href="https://zkx06111.github.io/">Kexun Zhang</a></li>
                <li><a href="https://weirayao.github.io/">Weiran Yao</a></li>
                <li><a href="https://zuxin.me/">Zuxin Liu</a></li>
                <li><a href="https://www.linkedin.com/in/yihao-feng-647985269/">Yihao Feng</a></li>
                <li><a href="https://sites.google.com/view/zhiwei-jim">Zhiwei Liu</a></li>
                <li><a href="https://www.linkedin.com/in/rithesh-r-n/?originalSubdomain=ca">Rithesh Murthy</a></li>
                <li><a href="https://www.linkedin.com/in/tian-lan-770b4b165/">Tian Lan</a></li>
                <li><a href="https://lileicc.github.io/">Lei Li</a></li> <br>
                <li><a href="https://renzelou.github.io/">Renze Lou</a></li>
                <li><a href="https://jiacheng-xu.github.io/">Jiacheng Xu</a></li>
                <li><a href="https://bpucla.github.io/">Bo Pang</a></li>
                <li><a href="https://www.linkedin.com/in/yingbozhou/">Yingbo Zhou</a></li>
                <li><a href="https://www.shelbyh.ai/">Shelby Heinecke</a></li> <br>
                <li><a href="https://www.linkedin.com/in/silvio-savarese-97b76114/">Silvio Savarese</a></li>
                <li><a href="https://huan-december.github.io/">Huan Wang</a></li>
                <li><a href="http://cmxiong.com/">Caiming Xiong</a></li>
                <br>
                <br>
                    <a href="https://www.salesforceairesearch.com/">
                        <image src="img/salesforce-research.png" height="40px"> 
                        Salesforce AI Research &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    </a>
                    <a href="https://www.cmu.edu/">
                        <image src="img/cmu.png" height="40px"> 
                        Carnegie Mellon University &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    </a>
                </ul>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="mailto:kexun@cmu.edu">
                    <img src="img/paper_small.png" height="60px" alt="Paper">
                    <h4><strong>Paper</strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="mailto:kexun@cmu.edu">
                <image src="img/github.png" height="60px">
                <h4><strong>Code (Coming Soon) </strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="https://github.com/zkx06111/experiments/tree/main/evaluation/lite/20240727_salesforce_research_ensemble_agents">
                    <img src="img/data.jpg" height="60px" alt="Data">
                    <h4><strong>Trajectory </strong></h4>
                </a>
            </div>
            <div class="col-md-2 text-center">
                <a href="#leaderboard">
                <image src="img/results.svg" height="60px">
                <h4><strong>Leaderboard Results</strong></h4>
                </a>
            </div>
        </div>

        <br>
        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <figure style="text-align:center;">
                    <img src="img/swecommittee_teaser.jpg" width="100%">
                    <figcaption><b>Figure 1:</b> Different SWE agents (Aider, Moatless, Agentless, OpenDevin) resolve very different sets of issues (the colored girds), despite having similar resolve rates (b). Our proposed <b>DEI</b> Committee takes candidates patches and tries to select the best, oracle choice (c), improving the resolve rate significantly to be better than any single agent in the committee.</figcaption>
                </figure>
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3 class="mt-4 mb-2">Abstract</h3>
                <p class="text-justify">
                    Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent framework can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose <b>DEI</b> (Diversity Empowered Intelligence), a framework that leverages their unique expertise. <b>DEI</b> functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a <b>DEI</b>-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a collection of open-source SWE agents with a maximum individual performance of 27.3% on SWE-Bench Lite can be boosted to 34.3% by DEI, beating most closed-source solutions on the leaderboard. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.
                </p>
            </div>
        </div>
        <br>
        <!-- Leaderboard Section -->
        <div class="container" id="leaderboard" style="background: #e5effc">
            <div class="col-md-12">
                <h5>SWE-Bench Lite Leaderboard Results</h5>
                <p class="text-justify" style="font-size: 15px;">
                    (Last Updated: 2024-08-12)
                </p>
                <p class="text-left">
                    We present the resolve rates of top-ranking submissions on SWE-Bench Lite. We evaluate 3 DEI Committees formed by different groups of agents. Each DEI Committee outperforms the best agent in it. DEIBASE-Open, a committee formed by 4 open-source agents can beat many closed-source agents.
                </p>
    <!--             <div style="margin-bottom: 15px;">
                    <button id="expand-btn" onclick="toggleExpand()">Expand/Collapse Table</button>
                    <span style="margin-left: 10px;"><b><i style="font-size: 1.0em;">Last Updated: 2024-07-28 <a href="https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard#changelog">[Change
                                    Log]</a></i></b></span>
                </div> -->
                <div class="table-container">
                    <table id="leaderboard-table">
                        <thead>
                            <tr>
                                <th class="summary-small-header" colspan="1">DEI Group</th>
                                <th class="summary-small-header" colspan="1">% Resolve</th>
                                <th class="summary-small-headerr" colspan="1">SWE Agent System</th>
                                <th class="summary-small-header" colspan="1">Open Source Code</th>
                                <th class="summary-small-header" colspan="1">Trajectories</th>
                                <th class="summary-small-header" colspan="1">Open Patch Candidates</th>
                                <th class="summary-small-header" colspan="1">Backend LLM</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="background-color: #6699cc;">
                                <td><b>1</b></td>
                                <td><b>55.0</b></td>
                                <td><b>Salesforce Research DEIBASE-1</b></td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>&#10007;</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr style="background-color: #b3cde3;">
                                <td>1</td>
                                <td>50.6</td>
                                <td><a href="https://cosine.sh/">Cosine Genie</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>Fine-tuned OpenAI GPT</td>
                            </tr>
                            <tr style="background-color: #b3cde3;">
                                <td>1</td>
                                <td>43.0</td>
                                <td><a href="https://aide.dev/">CodeStory Aide</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt4o + Claude 3.5 Sonnet</td>
                            </tr>
                            <tr>
                                <td>-</td>
                                <td>38.0</td>
                                <td><a href="https://mentat.ai/blog/mentatbot-sota-coding-agent">AbenteAI MentatBot</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr style="background-color: #ffb366;">
                                <td><b>2</b></td>
                                <td><b>37</b></td>
                                <td><b>Salesforce Research DEIBASE-2</b></td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>&#10007;</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr style="background-color: #66cc66;">
                                <td><b>Open</b></td>
                                <td><b>34.3</b></td>
                                <td><b>Salesforce Research DEI-Open</b></td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr>
                                <td>-</td>
                                <td>34.0</td>
                                <td><a href="https://www.marscode.com/">Bytedance MarsCode</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr>
                                <td>-</td>
                                <td>33.0</td>
                                <td><a href="https://arxiv.org/abs/2406.01422">Alibaba Lingma</a></td>
                                <td>&#10007;<sup>1</sup></td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt-4-1106-preview</td>
                            </tr>
                            <tr style="background-color: #ffcc99;">
                                <td>2</td>
                                <td>31.3</td>
                                <td><a href="https://www.factory.ai/">Factory Code Droid</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>Anthropic + OpenAI</td>
                            </tr>
                            <tr style="background-color: #ffcc99;">
                                <td>2</td>
                                <td>30.6</td>
                                <td><a href="https://autocoderover.dev/">AutoCodeRover</a></td>
                                <td>&#10007;<sup>2</sup></td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr style="background-color: #ffcc99;">
                                <td>2</td>
                                <td>29.6</td>
                                <td><a href="https://aws.amazon.com/q/developer/">Amazon Q Developer</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>Unknown</td>
                            </tr>
                            <tr style="background-color: #ffcc99;">
                                <td>2</td>
                                <td>28.3</td>
                                <td><a href="https://github.com/NL2Code/CodeR">CodeR</a></td>
                                <td>&#10007;<sup>1</sup></td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt-4-1106-preview</td>
                            </tr>
                            <tr style="background-color: #ffcc99;">
                                <td>2</td>
                                <td>28.0</td>
                                <td><a href="https://github.com/masai-dev-agent/masai">MASAI</a></td>
                                <td>&#10007;<sup>1</sup></td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>Unknown</td>
                            </tr>
                            <tr>
                                <td>-</td>
                                <td>27.6</td>
                                <td><a href="https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240706_sima_gpt4o">SIMA</a></td>
                                <td>&#10007;<sup>1</sup></td>
                                <td>&#10003;</td>
                                <td>&#10003;<sup>3</sup></td>
                                <td>gpt4o</td>
                            </tr>
                            <tr style="background-color: #ccffcc;">
                                <td>Open</td>
                                <td>27.3</td>
                                <td><a href="https://github.com/OpenAutoCoder/Agentless">Agentless</a></td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>-</td>
                                <td>gpt4o</td>
                            </tr>
                            <tr style="background-color: #ccffcc;">
                                <td>Open</td>
                                <td>26.6</td>
                                <td><a href="https://github.com/aorwall/moatless-tools">Moatless Tools</a></td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>-</td>
                                <td>Claude 3.5 Sonnet</td>
                            </tr>
                            <tr>
                                <td>-</td>
                                <td>26.6</td>
                                <td><a href="https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240612_IBM_Research_Agent101">IBM Research Agent</a></td>
                                <td>&#10007;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>Unknown</td>
                            </tr>
                            <tr style="background-color: #ccffcc;">
                                <td>Open</td>
                                <td>26.3</td>
                                <td><a href="https://github.com/paul-gauthier/aider">Aider</a></td>
                                <td>&#10003;</td>
                                <td>&#10007;</td>
                                <td>-</td>
                                <td>gpt4o + Claude 3 Opus</td>
                            </tr>
                            <tr style="background-color: #ccffcc;">
                                <td>Open</td>
                                <td>26.0</td>
                                <td><a href="https://docs.all-hands.dev/">OpenDevin + CodeAct</a></td>
                                <td>&#10003;</td>
                                <td>&#10003;</td>
                                <td>-</td>
                                <td>gpt4o</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p></p>
                <p>
                    <b>Note:</b>
                    <ul>
                        <li><sup>1</sup> Their repo has no code yet.</li>
                        <li><sup>2</sup> An earlier version is open-source. The current one is not.</li>
                        <li><sup>3</sup> Candidates are generated by a "modification of moatless tools".</li>
                    </ul>
                </p>
            </div>
        <div>
            <p></p>
        </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3 class="mt-4 mb-2">DEIBASE: A Simple Yet Powerful DEI framework</h3>
                <p class="text-justify">
                    We present <b>DEIBASE</b>, a simple yet powerful implementation of the DEI framework, tailored for SWE-Bench like problems. The context in the setup includes the repository, along with relevant files and issue descriptions. The meta-policy's action space consists of the final patches generated by different agent frameworks, each specialized in addressing various aspects of the problem.
                </p>

                <p class="text-justify">
                    <b>DEIBASE</b> utilizes a Large Language Model (LLM) as a code review committee. The LLM evaluates candidate patches by analyzing the state of the code base before and after the proposed changes, in conjunction with the contextual information from the issue descriptions. It produces detailed explanations for each patch, justifying the modifications based on the identified issues, the context, and the specific changes made.
                </p>


                <p class="text-justify">
                    While other methods of code review and scoring, such as rule-based approaches, can be incorporated into our framework, the use of an LLM-based committee offers a unique advantage. LLMs excel at evaluating solutions, which often requires less complexity than generating them. DEIBASE thus serves as an effective baseline for LLM-based SWE evaluation, highlighting potential performance variations among diverse SWE agents and showcasing the capabilities of our method.
                </p>

                <figure>
                    <img src="img/swecommittee_pipeline.jpg" alt="Framework Overview" style="width:100%;">
                    <figcaption><b>Figure 2:</b> Framework Overview. <b>DEI</b> first examines the code base before and after a candidate patch, along with other relevant contexts. Then, it generates an explanation for the issue, the context, and the patch and tries to justify the patch. With its own explanation, it scores the candidate patches and picks the top-scoring ones as more likely to be correct.</figcaption>
                </figure>

                <h5 class="mt-4 mb-2">Step 1: Input Construction</h5>
                <p class="text-justify">
                    Four inputs are given to <b>DEIBASE</b> for each patch: the issue description itself, relevant context (code snippets identified by an SWE agent as relevant to the issue), code before the patch, and code after the patch. The form of inputs reflects two design choices. First, the entire repository is often too large to fit directly in the context limit of LLMs, so we use the relevant context instead to save token costs and help the model focus. Second, the format of a patch is not the easiest for an LLM to read as it switches back and forth between the pre-change code and the changed code, so we give the code before and after the patch separately to the model for easier understanding. 

                <h5 class="mt-4 mb-2">Step 2: Explanation Generation</h5>
                <p class="text-justify">
                    To help the model better "understand" the patch before scoring, we instruct it to generate various explanations regarding the patch in a specified order. The order is decided so that the earlier explanations can also help the later ones. We describe each explanation in the order they are generated here:
                </p>
                <ul>
                    <li><b>Issue explanation:</b> Explains what the issue is about and what problem it may be causing.</li>
                    <li><b>Context explanation:</b> Explains how and why each relevant code span (there might be many of these) is relevant to the issue.</li>
                    <li><b>Location explanation:</b> Explains if and why the patch is modifying the correct part of the code that's faulty.</li>
                    <li><b>Patch explanation:</b> Explains if and how the patch is fixing the issue.</li>
                    <li><b>Conflict detection:</b> Checks whether the patch conflicts with other relevant code snippets.</li>
                </ul>
                <p class="text-justify">
                    We explicitly instruct the model to refer back to the earlier explanations while generating the later ones.
                </p>

                <h5 class="mt-4 mb-2">Step 3: Patch Scoring</h5>
                <p class="text-justify">
                    Based on its own explanations, the model is asked to give the candidate patch a score of 1 to 10. We give the model detailed rubrics of what violations/mistakes lead to higher score deduction and what should only be considered minor violations. For example, if the model finds the modification location to be wrong, it is considered a serious mistake.
                </p>

                <h3 class="mt-4 mb-2">Experiments</h3>
                <p class="text-justify">
                    We aim to answer two research questions with our experiments: 1) How diverse are LLM-based SWE agents in terms of intra- and inter-agent diversity? 2) To what extent can <b>DEI</b> harness the diversity and increase the performances of these SWE agents?
                </p>

                <h4 class="mt-4 mb-2">Experiment Setup</h4>
                <h5 class="mt-4 mb-2">Benchmark and Agents</h5>
                <p class="text-justify">
                    <b>Benchmark:</b> We conduct our experiments on SWE-Bench Lite, a 300-instance subset sampled from the full SWE-Bench for providing a more self-contained evaluation of functional bug fixes. Compared to the full SWE-Bench, SWE-Bench Lite has significantly more submissions on the leaderboard for us to conduct a more comprehensive analysis of inter-agent diversity.
                </p>
                <p class="text-justify">
                    <b>Agents:</b> For intra-agent diversity, we consider three well-performing open-source agents on the SWE-Bench Lite leaderboard: Agentless, Moatless, and Aider by running them 10 times with the same parameters. For inter-agent diversity, we consider 10 agents (open-source or not) that have similar resolve rates, all between 26.0% and 31.0% on the leaderboard by directly using their submitted patches to the SWE-Bench issues.
                </p>
                <p class="text-justify">
                    For the evaluation of <b>DEIBASE</b> on different agents, we consider 3 groups of agents that form different <b>DEI</b> Committees, including one group consisting of only open-source agents. For the evaluation of <b>DEIBASE</b> on multiple runs of a single agent, we use the generations of the three aforementioned agents -- Agentless, Moatless Tools, and Aider.
                </p>
                   
                <div class="text-center">
                    <div style="display: flex; justify-content: center; flex-wrap: wrap;">
                        <div style="flex: 0 0 40%; margin: 1.5%;">
                            <img src="img/multiagent_resolve_percentage.jpg" alt="Multiagent Resolve Percentage" style="width: 100%;">
                        </div>
                        <div style="flex: 0 0 40%; margin: 1.5%;">
                            <img src="img/agentless_resolve_percentage.jpg" alt="Agentless Resolve Percentage" style="width: 100%;">
                        </div>
                        <div style="flex: 0 0 40%; margin: 1.5%;">
                            <img src="img/aider_resolve_percentage.jpg" alt="Aider Resolve Percentage" style="width: 100%;">
                        </div>
                        <div style="flex: 0 0 40%; margin: 1.5%;">
                            <img src="img/moatless_resolve_percentage.jpg" alt="Moatless Resolve Percentage" style="width: 100%;">
                        </div>
                    </div>
                    <p class="text-justify" style="margin-top: 10px;">
                        <b>Figure 3:</b> How different metrics change as more candidate solutions are involved. In all 4 scenarios, there is a huge gap between Union@k and Average@k.
                    </p>
                </div>

                <h5 class="mt-4 mb-2">Evaluation Metrics</h5>
                <p class="text-justify">
                    We use the same set of metrics for both intra- and inter-agent diversity as these metrics are defined for multiple candidate solutions without requiring them to come from the same candidate:
                </p>
                <ul>
                    <li><b>Resolve rate:</b> Measures how good a SWE agent is. It is defined as the percentage of issues resolved by the agent. We measure both single SWE agents and <b>DEI</b> with it to see how much <b>DEI</b> helps.</li>
                    <li><b>Union@k:</b> Measures the <i>best case</i> performance had the agents been perfectly consistent by counting the number of problems solved by any of the k solutions.</li>
                    <li><b>Intersect@k:</b> Measures the <i>worst case</i> performance by computing the number of problems solved by all k solutions.</li>
                    <li><b>Average@k:</b> Measures the <i>average case</i> performance by computing the average number of problems solved. It corresponds to the case of a random reward function that uniformly samples a candidate solution for each problem.</li>
                    <li><b>n@k:</b> Measures the performance of any reranking mechanism by computing the number of problems solved by k chosen submissions from a given set of samples.</li>
                </ul>

                <p class="text-justify">
                    Our research questions can be answered by the gaps between these metrics. <b>Union@k - Intersect@</b> measures how diverse the agents are, while <b>n@k - Average@k</b> measures how much <b>DEI</b> helps in selecting the correct candidate from these agents. Note that the order -- in which different runs are added -- matters as k gets larger, especially when the k candidate solutions come from k different agents. In our experiments, we add candidate solutions from the single agent according to the order they are generated, while we add solutions from different agents in a fixed order.
                </p>

                <h4 class="mt-4 mb-2">How diverse are LLM-based SWE agents?</h4>
                <p class="text-justify">
                    To answer this question, we report the "@k" metrics of 10 different agents and 10 runs of single agents in <b>Figure 3</b>. Several observations can be made about the results:
                    <b>SWE agents resolve very different sets of issues across agents and agent runs. Their full potential is far from fully released.</b>
                    In all four subfigures, the gap between Union@k and Average@k, as well as between Average@k and Intersect@k, is large. 
                    As k—the number of candidates—gets larger, the gap also gets larger. In fact, Union@k is always more than 50% larger than Average@k for k≥5.
                    This indicates that current SWE agents are potentially capable of resolving a lot more issues, as long as we have a reranker that can tell which candidates are correct.
                    <b>Different agents resolve more distinct issues than different runs of a single agent. In other words, diversity does empower intelligence.</b>
                    The absolute/relative difference between Union@k and Average@k is much larger in the first subfigure than the following three subfigures.
                    As k approaches 10, the distinct issues resolved are 2× the average number of issues resolved by a single agent in the group.
                </p>             

                
            </div>
        </div>

    <div class="row justify-content-md-center">
        <div class="col-md-12 col-lg-10">
    <h4 class="mt-4 mb-2">How much does DEI help?</h4>
    <p class="text-justify">
        We apply <b>DEIBASE</b> to the candidates in <b>Figure 3</b> as they are added to the group. Our findings are:
    </p>
    <p class="text-justify">
        <b>DEIBASE helps in most cases.</b> For most values of k in all subfigures, we observe a significant improvement of n@k over Average@k, indicating that <b>DEIBASE</b> selects correct candidates much better than a random baseline.
    </p>
    <p class="text-justify">
        <b>DEIBASE helps more when the candidates come from different agents.</b> This finding resonates with the similar finding from research question one: Since candidates from multiple agents have a larger potential of improvement (Union@k - Average@k), the actual improvements created by <b>DEIBASE</b> (n@k - Average@k) are also larger. This suggests that given a limited budget of candidates, we should probably choose diversity of agents over multiple runs of the same agent.
    </p>
    <p class="text-justify">
        <b>As k gets larger, DEIBASE's improvement first increases and then plateaus.</b> While larger k generally indicates higher n@k, the margin gets smaller and there are cases when an increase in k results in a slight drop in performance. This suggests that the current <b>DEIBASE</b> is not ideal for a large group of agents and there is still room for a better reranking mechanism.
    </p>
    <p class="text-justify">
        Based on the lessons we summarize above, we propose three <b>DEIBASE</b> groups in which each candidate is from a different agent and no more than 5 candidates exist for each instance. <b>DEIBASE-1</b> consists of the top 2 agents. <b>DEIBASE-2</b> consists of 5 closed-source agents that have high performance on the leaderboard. <b>DEIBASE-Open</b> consists of 4 open-source agents so that we know future researchers can run the entire pipeline.
    </p>
    <p class="text-justify">
        We also evaluate <b>DEIBASE's</b> performance on these groups in <b>the leaderboard results table</b>. All three <b>DEIBASE</b> instances outperform the best candidate in the group. Surprisingly, <b>DEIBASE-Open</b> shows a 7% increase in resolve rates and beats most of the closed-source systems.
    </p>
        </div>
    </div>

    <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h3>
                    Citation 
                </h3>
                <a href="https://arxiv.org/abs/2408.07060">[arxiv version]</a>
                <div class="form-group col-md-12">
                    <textarea id="bibtex" class="form-control" readonly>
@misc{zhang2024diversityempowersintelligenceintegrating,
    title={Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents}, 
    author={Kexun Zhang and Weiran Yao and Zuxin Liu and Yihao Feng and Zhiwei Liu and Rithesh Murthy and Tian Lan and Lei Li and Renze Lou and Jiacheng Xu and Bo Pang and Yingbo Zhou and Shelby Heinecke and Silvio Savarese and Huan Wang and Caiming Xiong},
    year={2024},
    eprint={2408.07060},
    archivePrefix={arXiv},
    primaryClass={cs.SE},
    url={https://arxiv.org/abs/2408.07060}, 
}</textarea>
                </div>
            </div>
        </div>



    <div class="row justify-content-md-center mt-4">
        <div class="col-md-10 col-lg-8">
            <p class="text-justify" style="font-size: 10px;">
                This page is adapted from the template of <a href="https://video-language-planning.github.io/">Video Language Planning</a> project website. We thank the authors for providing the template.
            </p>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
